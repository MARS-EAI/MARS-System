export const EN = {
    'tab.all': 'All',
    'tab.tool': 'Tool',
    'tab.data': 'Data',
    'nav.safeBase': 'Safety Foundation',
    'nav.safeTech': 'Safety Technology',
    'nav.safeEval': 'Safety Evaluation',
    'nav.safeService': 'Safety Services',
    'home.banner.desc': 'A Safety Ecosystem Platform Developed by Shanghai AI Lab',
    'home.block1.title': 'AI45: A Path to Trustworthy AGI',
    'home.block1.desc': 'A practical path to trustworthy AGI, inspired by Judea Pearl\'s "Ladder of Causation", the balanced development between capability and safety',
    'home.block1.inner1.title': 'AI-45° Law',
    'home.block1.inner1.content.p1': 'From a long-term perspective, AI safety and performance should ideally advance in parallel along a 45° line. Short-term fluctuations are permissible, but in the long run, this balance should neither fall below 45° (as at present) nor exceed it (to avoid constraining development).',
    'home.block1.inner1.content.p2': 'Multiple technical pathways may achieve this “AI-45° Law”. We are exploring a causality-centered approach—“the Causal Ladder of Trustworthy AGI"—spanning three progressive layers: Approximate Alignment Layer, Intervenable Layer, and Reflectable Layer.',
    'home.block1.inner1.con.red': 'Red Lines of AGI Development: Systematic AI  behaviors that may lead to irreversible and catastrophic consequences',
    'home.block1.inner1.con.yellow': 'Yellow Lines of AGI Development: An early warning threshold framework to signal when AI system capabilities approach dangerous levels',
    'home.block1.inner2.title': 'The Matrix of Trustworthy AGI',
    // eslint-disable-next-line max-len
    'home.block1.inner2.content': 'Based on the Causal Ladder of Trustworthy AGI, we prospectively define five levels of AGI trustworthiness: Perception Trustworthiness, Reasoning Trustworthiness, Decision-making Trustworthiness, Autonomy Trustworthiness, and Collaboration Trustworthiness. These levels correspond to varying dependencies on the three layers of the Ladder of Causation, with reliance on the Reflective Layer increasing as trustworthiness advances.',
    'home.block2.title': 'Functional Modules',
    // eslint-disable-next-line max-len
    'home.block2.base.desc': 'Explore behavioral boundaries, verifiability, formal modeling and analysis to support safety technologies, evaluation, and services.',
    'home.block2.tech.desc': 'Integrate endogenous and exogenous safety, focusing on trustworthy mechanisms, alignment, and agents.',
    'home.block2.eval.desc': 'Assess model safety with self-built toolchain， exploring model traceability via watermarking, authentication, and deepfake detection.',
    'home.block2.service.desc': 'Support model registration, evaluation, standardization; open-source resources across healthcare, education, finance, etc.',
    'home.block3.title': 'Open Source',
    'home.block3.data': 'Optimized Safety Data',
    'home.block3.data.text': 'Text',
    'home.block3.data.img': 'Image',
    'home.block3.tool': 'Safety Toolchain',


    'home.slider1.title': 'Zhou Bowen: Exploring the AI-45° Law',
    'home.slider1.desc': 'During the 2024 World Artificial Intelligence Conference(WAIC), Zhou Bowen, chief scientist of Shanghai AI Lab, introduced the original technical proposition of the "45° Law" that balancing AI safety and performance, and the Ladder of Causality for Trustworthy AGI. This contributes to China’s deep involvement in global science and technology governance, drawing strong responses both domestically and internationally.',
    'home.slider2.title': 'AI Safety Governance Closed-Door Meeting',
    'home.slider2.desc': 'WAIC-AI Safety Governance Closed-Door Meeting, with 18 institutions and 34 guests participating, promoting the construction of an international platform and professional collaboration network for AI safety and governance.',
    'home.slider3.title': 'International AI Safety Frontier Technology Forum',
    'home.slider3.desc': 'WAIC-International AI Safety Frontier Technology Forum, focusing on key technical issues in AI safety, discussing defense strategies, and exploring the future of creating socially responsible AGI from an ethical perspective.',
    'home.slider4.title': 'Workshop on Trustworthy Multi-modal Foundation Models and AI Agents',
    'home.slider4.desc': 'Shanghai AI Lab, together with Concordia AI, co-hosted the ICML 2024 Workshop on \'Trustworthy Multimodal Foundation Models and AI Agents\' in Vienna, Austria.The workshop featured discussions and roundtables on model robustness, safety, privacy protection, alignment techniques, social technology, and AI safety governance, bringing together valuable experiences from both industry and academia.',
    'home.slider5.title': 'The Release of AI Safety as Global Public Goods',
    'home.slider5.desc': 'On July 5, during the "2024 World Artificial Intelligence Conference and High-level Meeting on Global Governance of Artificial Intelligence", working report on "AI Safety as Global Public Goods" was released for the first time at the "Accelerating \'Artificial Intelligence + \' to Build New Quality Productivity" forum held at the West Bund Intelligence Tower in Xuhui District, Shanghai, and an initiative to establish an AI safety and governance dialogue network was launched.',
    'home.slider6.title': 'Puyuan Challenge (Summer Season) Adds Safety Trustworthy Track',
    'home.slider6.desc': 'Puyuan Challenge (Summer Season) introduced a new Safety Trustworthy track, aiming to explore the application and safety governance paths of large models across various fields. The addition of the Safety Trustworthy track was one of the highlights of the competition, attracting 90 teams. Seven teams from institutions like the University of Chinese Academy of Sciences, Fudan University, and individual developers shared their understanding of security risks and evaluation solutions for the LLMs era, providing insights and new technological development paths for enhancing the trustworthiness, automation, low-cost, high-reliability continuous monitoring, and updates in AI models.',
    'home.slider.learnMore': 'Learn More',

    'base.banner.title': 'Safety Foundation',
    'base.banner.subTitle': 'Build Theoretical Framework for Large Model Safety',
    'base.banner.desc': 'Deepen foundational theoretical research on large model safety, exploring the underlying principles and methods, and providing a solid theoretical foundation and support for safety technologies, evaluation, and services.',
    'base.module1.title': 'Causal Foundations for Large Model Safety',
    'base.module1.desc': 'Develop the foundational theory for large model safety grounded in causal science.',
    'base.module1.card1.title': 'Foundations of Causality for Large Models',
    'base.module1.card1.desc': 'Establish a finegrained and unified definition of causality for large models, laying the foundation for building safe multimodal large models, embodied agents, and world models.',
    'base.module1.card2.title': 'Foundations of Interpretability for Large Models',
    'base.module1.card2.desc': 'Establish a causal interpretability framework for multimodal large models, uncovering hallucination mechanisms and advancing interpretability research.',
    'base.module1.card3.title': 'Foundations of Self-Awareness in Large Models',
    'base.module1.card3.desc': 'Establish a causal definition of self-awareness, exploring how to quantify, represent, and manipulate self-awareness in large models, laying the foundation for large model safety.',


    'tech.banner.title': 'Safety Technology',
    'tech.banner.subTitle': 'Build a Safety Technology System for Large Models: Combining Endogenous and Exogenous Safety Technologies',
    'tech.banner.desc': 'We are committed to understanding the internal mechanism of the model, and achieving the coexistence of mechanism analysis and performance improvement through the combination of internal and external safety technologies.',
    'tech.module1.title': 'Mechanism Trustworthiness',
    'tech.module1.card1.title': 'Unveiling the Pre-training Process of Large Models',
    'tech.module1.card1.desc': 'Committed to understanding the trustworthy modeling process of LLMs during the pre-training phase, and for the first time, validating that slices from the pre-training process can help improve the final trustworthiness of LLMs.',
    'tech.module1.card2.title': 'Mitigating Internal Neuron Conflicts',
    'tech.module1.card2.desc': 'Addressing the trade- off phenomenon between privacy awareness and fairness awareness caused by SFT, a method called DEAN was proposed to mitigate this issue without retraining.',
    'tech.module1.card3.title': 'Exposing Potential Risks of Large Model Personality',
    'tech.module1.card3.desc': 'Revealing the potential safety risks associated with LLM personality customization, delving into the exact relationship between LLM\'s personality traits and its safety capabilities(e.g., toxicity, privacy, and fairness).',
    'tech.module2.title': 'Safety Alignment',
    'tech.module2.desc': 'Researching safety alignment methods such as training alignment and inference-time alignment',
    'tech.module2.card1.title': 'Align Large Language Models via Searching over Small Language Models',
    'tech.module2.card1.desc': 'Chunk search during inference can achieve value guidance and safety alignment in models.',
    'tech.module2.card2.title': 'Disalignment Alignment: Safety Leaks in Post-training stage',
    'tech.module2.card2.desc': 'Existing post-training safety alignment methods may have safety leaks!',
    'tech.module2.card3.title': 'Inference-Time Language Model Alignment via Integrated Value Guidance',
    'tech.module2.card3.desc': 'Use implicit and explicit value functions to guide language model decoding.',
    'tech.module3.title': 'Trustworthy Agents',
    'tech.module3.card.title': 'Simulating Safety Risks of Misinformation Spread',
    'tech.module3.card.desc': 'Simulate safety risks of misinformation spread within a multi-agent system based on LLMs, validated in scenarios such as public opinion manipulation and rumor spreading.',


    'eval.banner.title': 'Safety Evaluation',
    'eval.banner.subTitle': 'Build a Large Model Safety Evaluation System, Supporting Large Models Registration Evaluation',
    'eval.banner.desc': 'Self-developed evaluation frameworks and high-quality evaluation datasets, covering various modalities such as text, images, and videos, including open-ended questions, multiple choices, and jailbreak scenarios. Self-developed scoring model and evaluation platform, supporting automated evaluation and result analysis. Conduct trustworthy traceability research such as watermarking, shelling, and deepfake detection.',
    'eval.module1.title': 'Overall Layout',

    'eval.module2.title': 'Large Model Safety Evaluation',
    'eval.module2.tabModule1.title': 'Large Language Models',
    'eval.module2.tabModule1.tab1': 'Evaluation Benchmarks',
    'eval.module2.tabModule1.tab1.title': 'Text-to-Text',
    'eval.module2.tabModule1.tab1.desc': 'Systematically evaluating the potential safety risks of generated content by having the model produce text based on text descriptions.',
    'eval.module2.tabModule1.innerSlider1.item1.desc': 'FLAMES Evaluation ',
    'eval.module2.tabModule1.innerSlider1.item1.intro': 'A highly adversarial Chinese LLMs value trend evaluation benchmark, including 2k+ manually designed data covering five value dimensions, along with an associated scoring model',
    'eval.module2.tabModule1.innerSlider1.item2.desc': 'Fake Alignment Evaluation',
    'eval.module2.tabModule1.innerSlider1.item2.intro': 'By comparing the results of multiple-choice questions and open-ended questions, the Fake Alignment problem in LLMs is proposed, and a novel automated Fake Alignment evaluation framework is designed.',
    'eval.module2.tabModule1.innerSlider1.item3.desc': 'Salad-Bench Benchmark',
    'eval.module2.tabModule1.innerSlider1.item3.intro': 'An open-source dataset containing 66 safety-related dimensions, which can evaluate LLM safety, attacks, and defense methods revealing the actual safety of LLMs across various security dimensions.',
    'eval.module2.tabModule1.tab2': 'Evaluation Models',
    'eval.module2.tabModule1.tab2.title': 'Text-to-Text',
    'eval.module2.tabModule1.tab2.desc': 'Efficient, automated scoring for safety-related question-answer pairs.',
    'eval.module2.tabModule1.innerSlider2.item1.desc': 'MD-Judge-v0.2-internlm2_7b Model',
    'eval.module2.tabModule1.innerSlider2.item1.intro': 'An open-source safety evaluation model for question-answer pairs, judging whether the question-answer pair is safe and which safety dimension is violated.',

    'eval.module2.tabModule2.title': 'Multimodal Large Models',
    'eval.module2.tabModule2.tab1': 'Image (Text) to Text',
    'eval.module2.tabModule2.tab1.title': 'Image (Text) to Text',
    'eval.module2.tabModule2.tab1.desc': 'This includes object recognition or text description of images (with text) to test whether the model generates unsafe content.',
    'eval.module2.tabModule2.tab2': 'Text to Image',
    'eval.module2.tabModule2.tab2.title': 'Text to Image',
    'eval.module2.tabModule2.tab2.desc': 'Generate an image through a prompt (text description) to test if the model outputs unsafe content.',
    'eval.module2.tabModule2.innerSlider1.item1.desc': 'MLLMGuard Evaluation',
    'eval.module2.tabModule2.innerSlider1.item1.intro': 'A multidimensional safety evaluation benchmark for MLLMs in both Chinese and English, including a high-quality bilingual image-text evaluation dataset, inference utilities, and a set of lightweight scoring models.',
    'eval.module2.tabModule2.innerSlider1.item2.desc': 'VLSBench Evaluation',
    'eval.module2.tabModule2.innerSlider1.item2.intro': 'The current multimodal security benchmark reveals a visual information leakage issue, leading to shortcuts in multimodal safety alignment. We constructed 2k multimodal security dataset without visual information leakage, and current MLLMs perform poorly on this data.',
    'eval.module2.tabModule2.innerSlider1.item3.desc': 'MM-Safetybench Evaluation',
    'eval.module2.tabModule2.innerSlider1.item3.intro': 'A novel visual instruction attack is proposed to jailbreak an open-source LMM using query-related images. Based on keywords extracted from malicious queries, a composite image is created from an image generated by a diffusion model and another displaying text in a typographic format.',

    'eval.module2.tabModule2.innerSlider2.item1.desc': 'T2ISafety Benchmark',
    'eval.module2.tabModule2.innerSlider2.item1.intro': 'We propose unsafe dimensions in Text-to-Image, such as fairness, toxicity, privacy, and their subcategories, construct corresponding image description datasets to test the safety of Text-to-Image models. We also develop a safety evaluation model ImageGuard for synthetic/real images to automatically evaluate generated content\'s safety',

    'eval.module3.title': 'Trustworthy Traceability',
    'eval.module4.title': 'Jailbreak Attacks',
    'eval.module4.desc': 'Tests whether the model generates harmful text content through attacks, including in areas such as question-answering, dialogue, and programming, etc.',
    'eval.module3.card1.title': 'LLM Watermarking',
    'eval.module3.card1.desc': 'Offer lightweight, privacy-preserving watermarking for LLMs by adjusting only the logits generation, enabling high-density watermarking without compromising output quality.',
    'eval.module3.card2.title': 'Shell Detection',
    'eval.module3.card2.desc': 'Given a model\'s intermediate representation, perform base model traceability detection to determine if the model was fine- tuned based on an existing base model.',
    'eval.module3.card3.title': 'Deepfake Detection',
    'eval.module3.card3.desc': 'Provide deepfake detection services for digital content—whether text, image or video—to verify whether it was produced by a generative model or originated from a real source.',
    'eval.module4.card1.title': 'CodeAttack',
    'eval.module4.card1.desc': 'We propose the jailbreak attack method CodeAttack, which disables the safety barriers of large language models in the code domain. The attack success rate of more than 80% was achieved on state-of-the-art LLMs (including GPT, Claude, and Llama series).',
    'eval.module4.card2.title': 'ActorAttack',
    'eval.module4.card2.desc': 'We designed a multi-round attack method ActorAttack, successfully attacked models such as OpenAI o1, and proposed the first multi-round secure alignment dataset.',
    'service.banner.title': 'Safety Services',
    'service.banner.subTitle': 'One-stop Large Model Safety Services',
    'service.banner.desc': 'Provide comprehensive large model safety solutions, supporting large model registration evaluation and standard construction. Develop high-quality multimodal safety datasets, covering vertical fields such as healthcare, education, and finance.  Create and open-source industry-leading large model safety toolchains.',
    'service.module1.title': 'Open-Source Services',

    'service.module1.card.ADCE.title': 'ADCE',
    'service.module1.card.ADCE.desc': 'Define computable, explainable metrics for LLMs\' semantic understanding.',

    'service.module1.card.ActorAttack.title': 'ActorAttack',
    'service.module1.card.ActorAttack.desc': 'Multi-round dialogue attack method for LLMs.',

    'service.module1.card.CodeAttack.title': 'CodeAttack',
    'service.module1.card.CodeAttack.desc': 'Single-round code attack method for LLMs.',

    'service.module1.card.CELLO.title': 'CELLO',
    'service.module1.card.CELLO.desc': 'The first multimodal LM causal reasoning ability benchmark.',

    'service.module1.card.CredID.title': 'CredID',
    'service.module1.card.CredID.desc': 'Easily add hidden watermark messages to LLM responses.',

    'service.module1.card.CaLM.title': 'CaLM',
    'service.module1.card.CaLM.desc': 'The first evaluation system for trustworthy causal reasoning in LMs.',

    'service.module1.card.DEAN.title': 'DEAN',
    'service.module1.card.DEAN.desc': 'Simultaneously improves fairness and privacy in LLMs w/o training.',

    'service.module1.card.ESCEval.title': 'ESC-Eval',
    'service.module1.card.ESCEval.desc': 'Evaluating emotional companionship models.',

    'service.module1.card.Emulated.title': 'Emulated Disalignment',
    'service.module1.card.Emulated.desc': 'Alignment methods may have security leakage issues!',

    'service.module1.card.FLAMES.title': 'FLAMES',
    'service.module1.card.FLAMES.desc': 'High-adversity Chinese LLMs value evaluation benchmark.',

    'service.module1.card.Fake.title': 'Fake Alignment',
    'service.module1.card.Fake.desc': 'The first evaluation framework for fake alignment',

    'service.module1.card.MLLMGuard.title': 'MLLMGuard',
    'service.module1.card.MLLMGuard.desc': 'Safety benchmark for MLLM.',

    'service.module1.card.MMSafetyBench.title': 'MM-SafetyBench',
    'service.module1.card.MMSafetyBench.desc': 'Safety evaluation dataset and leaderboard for multimodal large models.',

    'service.module1.card.MODPO.title': 'MODPO',
    'service.module1.card.MODPO.desc': 'Multi-objective preference optimization',

    'service.module1.card.MORE.title': 'MORE',
    'service.module1.card.MORE.desc': 'The first benchmark to evaluate single-modality bias in MLMs.',

    'service.module1.card.MDJudge.title': 'MD-Judge',
    'service.module1.card.MDJudge.desc': 'An open-source safety evaluation model.',

    'service.module1.card.Persafety.title': 'Persafety',
    'service.module1.card.Persafety.desc': 'Enhancing safety by modifying LLMs personalities w/o training.',

    'service.module1.card.ReflectionBench.title': 'Reflection-Bench',
    'service.module1.card.ReflectionBench.desc': 'Evaluating reflection from a cognitive science perspective.',

    'service.module1.card.REEF.title': 'REEF',
    'service.module1.card.REEF.desc': 'A method for identifying base models without training.',

    'service.module1.card.SelfConsciousness.title': 'SelfConsciousness',
    'service.module1.card.SelfConsciousness.desc': ' First formal definition of LLM self-consciousness.',

    'service.module1.card.SALADBENCH.title': 'SALAD-BENCH',
    'service.module1.card.SALADBENCH.desc': 'A versatile safety evaluation for LLMs.',

    'service.module1.card.SEER.title': 'SEER',
    'service.module1.card.SEER.desc': 'RL-based structured reasoning performance optimization.',

    'service.module1.card.TraceLLM.title': 'TraceLLM',
    'service.module1.card.TraceLLM.desc': 'Explain the pre-training process of large models.',

    'service.module1.card.T2ISafety.title': 'T2ISafety',
    'service.module1.card.T2ISafety.desc': 'Safety evaluation in Text-to-Image models.',

    'service.module1.card.VLSBench.title': 'VLSBench',
    'service.module1.card.VLSBench.desc': 'Safety evaluation dataset for multimodal large models.',

    'service.module1.card.Weak-to-Strong.title': 'Weak-to-Strong Search',
    'service.module1.card.Weak-to-Strong.desc': 'Chunk search enables safety alignment .',

    'service.module2.title': 'Large Model Registration Evaluation',
    'service.module2.card1.title': 'Systematic and Scientific Evaluation Framework',
    'service.module2.card1.desc': 'Build an interdisciplinary network to create a framework integrating global values and domestic regulations.',
    'service.module2.card2.title': 'Massive-scale Safety Data',
    'service.module2.card2.desc': 'Build large-scale evaluation dataset with thousands of dimensions, multiple modalities, and diverse formats.',
    'service.module2.card3.title': 'Efficient and Accurate Evaluation Models',
    'service.module2.card3.desc': 'Self-develop efficient automated evaluation models, allowing for model-to-model evaluations.',
    'service.module2.card4.title': 'Full-process Evaluation Platform',
    'service.module2.card4.desc': 'Develop a platform with automated testing, labeling, red team attacks, and report auto-generation.',
    'service.module3.title': 'Vertical Field Evaluations',
    'service.module3.card1.title': 'Healthcare',
    'service.module3.card1.desc': 'Model Evaluation on risks in healthcare field, detecting inappropriate responses in medical ethics and general safety issues, such as leaking patient privacy or outputting incorrect knowledge.',
    'service.module3.card2.title': 'Education',
    'service.module3.card2.desc': 'Model Evaluation on risks in education field, detecting inappropriate content such as spreading harmful educational concepts or providing incorrect knowledge.',
    'service.module3.card3.title': 'Media',
    'service.module3.card3.desc': 'Model Evaluation on risks in media industry, detecting false information spreading, defamation, or rumor-mongering.',
    'service.module3.card4.title': 'Finance',
    'service.module3.card4.desc': 'Model Evaluation on risks in financial field, detecting issues such as commercial illegal activities or providing financial investment advice.',
    'service.module3.card5.title': 'Manufacturing',
    'service.module3.card5.desc': 'Model Evaluation on risks in manufacturing field, detecting issus like leaking business secrets or providing unsafe production suggestions.',
    'service.module3.card6.title': 'Urban Governance',
    'service.module3.card6.desc': 'Model Evaluation on risks in urban governance, detecting issues like leaking sensitive city management information or publishing incorrect public policy suggestions.',


    'nav.safeBaseNew': 'Embodied Multi-Agent Technology',
    'nav.safeTechNew': 'Data and Development Platform',
    'nav.safeEvalNew': 'Research Projects',
    'nav.safeServiceNew': 'About Us',

    'home.banner.descNew': 'MARS Platform led and developed by Shanghai Artificial Intelligence Laboratory',

    'base.banner.titleNew': 'Embodied Multi-Agent Technology',
    'base.banner.subTitleNew': ' ',
    'base.banner.descNew': ' ',
    'base.module1.titleNew': 'Combinatorial Constraints',
    'base.module1.descNew': ' ',
    'base.module1.card1.titleNew': 'Foundations of Causality for Large Models',
    'base.module1.card1.descNew': 'Provides the first definition of causal relationships for entities in multimodal large models, laying the theoretical foundation for building safe and trustworthy multimodal large models, embodied intelligent models, and world models with causal reasoning capabilities.',
    'base.module1.card2.titleNew': 'Causal Interpretability Framework for Large Models',
    'base.module1.card2.descNew': 'Develops the first causal interpretability framework for multimodal large models, exploring the mechanisms behind hallucination problems and laying the groundwork for future explainability studies of large models.',
    'base.module1.card3.titleNew': 'Foundations of Self-Awareness for Large Models',
    'base.module1.card3.descNew': 'Provides the first formal mathematical definition of self-awareness, and explores how to quantify, represent, and manipulate self-awareness in large models, establishing the basis for advanced safety and trustworthiness research in large models.',
    'base.module2.titleNew': 'Combinatorial Generalization',
    'base.module2.descNew': ' ',
    'base.module3.titleNew': 'Collective Brain',
    'base.module3.descNew': ' ',

    'tech.banner.titleNew': 'Data and Development Platform',
    'tech.banner.subTitleNew': ' ',
    'tech.banner.descNew': ' ',
    'tech.module1.titleNew': 'Data Factory',
    'tech.module2.titleNew': 'Embodied Collective Intelligence Simulation Platform',
    'tech.module3.titleNew': 'Human-Machine Interaction',
    'tech.module4.titleNew': ' ',

    'eval.banner.titleNew': 'Research Projects',
    'eval.banner.subTitleNew': ' ',
    'eval.banner.descNew': ' ',
    'eval.module1.titleNew': 'Robot Manipulation',
    'eval.module2.titleNew': 'Robot Navigation',
    'eval.module3.titleNew': 'World Models',
    'eval.module4.titleNew': 'Trustworthy Embodiment',

    'service.banner.titleNew': 'About Us',
    'service.banner.subTitleNew': 'We are building a real-world embodied society with our partners',
    'service.banner.descNew': ' ',
    'service.module3.titleNew': 'News',
    'service.module3.card1.titleNew': 'News 1',
    'service.module3.card1.descNew': 'News content news content news content news content news content news content news content news content news content news content news content news content news content news content news content.',
    'service.module3.card2.titleNew': 'News 2',
    'service.module3.card2.descNew': 'News content news content news content news content news content news content news content news content news content news content news content news content news content news content news content.',
    'service.module3.card3.titleNew': 'News 3',
    'service.module3.card3.descNew': 'News content news content news content news content news content news content news content news content news content news content news content news content news content news content news content.',
    'service.module3.card4.titleNew': 'News 4',
    'service.module3.card4.descNew': 'News content news content news content news content news content news content news content news content news content news content news content news content news content news content news content.',
    'service.module3.card5.titleNew': 'News 5',
    'service.module3.card5.descNew': 'News content news content news content news content news content news content news content news content news content news content news content news content news content news content news content.',
    'service.module3.card6.titleNew': 'News 6',
    'service.module3.card6.descNew': 'News content news content news content news content news content news content news content news content news content news content news content news content news content news content news content.',


};
